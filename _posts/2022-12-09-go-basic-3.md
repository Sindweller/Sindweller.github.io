---
layout: default
title: 2022/12/09 golang基础3 【学习笔记】
author: sindweller <sindweller5530@gmail.com>
tags: [编程语言]
---

## go的包管理
### vendor
1.6版本后，可以在go项目的vendor目录里，按照同样的规范（github.com/）把依赖包放进去，每个项目维护自己依赖的第三方包版本，直接把源码放进去。
### vendor管理工具
通过声明式配置，实现自动管理依赖  
切换gomod开启方式： `export GO111MODULE=on`  
只需要下载依赖包的一部分。  
依赖管理工具可以防止胡乱篡改依赖包。 
## 使用go mod
可以通过命令的形式：`go mod init`  
加新的依赖包： `go mod tidy`  
把源码拷贝到vendor目录里： `go mod vendor`  
定制化： `replace ` 一般定制依赖包版本  
## 代理/私有仓库
- 为go依赖设置代理 `export GOPROXY=https://goproxy.cn`  
- 设置私有代码仓库 `GOPRIVATE=*.corp.example.com`

## 网络基础
- 链路层： 使用mac地址通信
- 网络层： 网络传输：包根据ip转到ip
- 传输层：根据端口来处理
- socket抽象层： syscall:bind 0.0.0.0:80 内核态 应用发的包需要通过socket从内核态传出
- 应用层： app listen：0.0.0.0:80 本机任何ip都可以访问 用户态
app发出的包->传输层加tcp头->网络层加ip头
  
## socket
- 计算机之间通信的约定方式
- linux的一切都是文件
- 为了区分已打开的文件，linux会给每个文件分配一个ID（整数），即文件描述符
- 网络连接也是文件，也有文件描述符
- server
  - 初始化socket
  - bind 绑定端口
  - listen 监听端口
  - accept 调用阻塞，等待客户端连接
- client
  - 初始化socket
  - connect 连接服务器
- 连接成功，则server与client建立连接
## go的http
net.http包  
- 注册handler处理函数 `http.HandleFunc("/healthz", healthz)` 表明要以什么样的逻辑处理请求
```go
func healthz(w http.ResponseWriter, r *http.Request){
    io.WriteString(w, "ok") // 直接用io写回去
}
```
- 绑定端口：`http.ListenAndServe(":80", nil)` 
## 阻塞IO模型
整个系统调用链路很长：
应用进程 `recvfrom` --系统调用--> 系统内核（数据报文尚未就绪）->数据报文就绪 ->拷贝数据 -> 拷贝完成 --系统调用返回--> 处理数据报  
进程因为系统调用陷入内核态，阻塞于recvfrom调用。其实不利于程序性能。
## 非阻塞IO模型
轮询，系统调用后如果内核报文未就绪，返回错误，继续执行其他的。
## 异步IO
发起系统调用，直接返回，内核拷贝完成再告诉应用进程。
## 实际：IO多路复用
应用进程可以起一个线程，集中处理IO请求，会把当前已经建立连接的所有文件描述符组成数组，供系统调用来遍历。内核会检查数组中的文件描述符哪些就绪哪些没就绪，然后返回给应用进程。应用进程就会去处理已就绪的文件描述符。  
限制：应用态到内核态传递的参数长度有限，不能传过多的fd。
### 改进：多路复用机制 epoll
- epoll_create1 结构：
  - wq
  - rdlist，存就绪epitem，readylist，是一个链表
  - rbr 一个红黑树，
- epoll_ctl(epfd,op,fd,event) 应用程序通过这个系统调用将指定的fd注册到内核态，且注册监听的事件，以及事件发生之后处理handler是什么。
- epoll_wait(epfd,event, maxevents,timeout) 用户态调用线程去读连接数据，如果从上面的rdlist中读出的fd（数据）未就绪，则调用这个wait让线程阻塞，然后这个线程可以做别的事情。
也就是所有连接的fd都会构建到epitem（rbr）这个红黑树里。  
当从network stack（协议栈）里收到了数据包，会去红黑树里查找这个数据包对应哪个连接。查找到之后会把这个已经就绪的epitem放到rdlist里。通过维护rdr和rdlist，就减少了用户态和内核态的交互数据量。通过事件通知的机制提升效率。
### go依赖epoll实现高性能http server
- go协程与fd资源绑定
  - socket fd未就绪，协程设置为Gwaiting状态，CPU时间片让渡给其他协程
  - runtime调度器进行调度唤醒协程时，检查fd是否就绪，如果就绪则协程状态置为runnable并加入执行队列
  - 协程被调度后，处理fd数据

## 容器和虚拟机相比的优势
内核
- 进程调度
- 内存管理
- 文件系统
- 网络协议栈
容器技术
- ns
- cgroup
- overlayfs
- 网络
- 实践
运维能力
- 系统配置
- 问题排查
- shell脚本
## label 和 annotation的区别
实际上是问k8s对象的设计原则的理解。
- 都是map，为对象存储一些附加信息
- annotation是k8s程序里会去识别的，不能用来筛选查询，是为对象增加额外属性，label更偏向于用户来在apiserver端进行筛选。
## resourceVersion是什么
对k8s版本控制的理解。实际在问对etcd的理解。etcd每次对对象做修改的时候，会增加一个版本号。  
他相当于一个乐观锁，当前版本是1，如果有两个controller要去修改这个对象，那么会读到1这个版本号，其中一个控制器修改对象，请求提交给apiserver，apiserver查看是否是基于当前版本1的更新，如果是，则会commit并改为2。
## k8s考点
- 认证，与企业认证平台集成
- 鉴权，生产系统权限规划
- 准入，配额控制，多个部门都要用，如何确保不让一个部门都用光
- 缓存，大规模集群下的缓存配置
### 核心组件kubelet
- pod manager （插件如何配置）
  - CRI 运行时接口
  - CNI 网络接口
  - CSI 存储接口
- probe manager
- oom manager
## 控制器
- deployment controller （pod template hash，pod deployment流程的理解）
- statefulset controller
## kube-proxy 代理
对内核实现的理解
- iptables
- ipvs

内存水位-pod驱逐
## 高可用集群
对每个核心组件的高可用
## istio spring cloud的优劣比较
k8s对服务治理有劣势，通过istio去弥补。
- spring cloud只能使用java
- istio所有通用机制从代码中抽出来，而不是像springcloud一样把jar包放到每个项目里，将业务代码和流控代码（服务管理代码）解耦。
- istio写yaml文件下发即可。
### 概念
sidecar流量劫持原理

## go如何debug
- gdb gccgo原生支持，但对go语言的栈管理、多线程支持等方面做的不太好，debug时可能会有错乱
- dlv go的专属debugger goland的debug功能就默认是dlv
### dlv使用
debug 模式运行run.go  
打断点后再运行，可以看到调用栈和函数中的变量。例如r是http request，那么就可以看到他带过来的header等信息。 
### log
- 如果不需要额外以来，可以用fmt.Println。但只能往标准输出打，没有重定向和日志分级
- 日志框架，glog
  - 多级别
  - 自带时间戳和代码行
  - 可配置appender，标准输出转到文件
> glog 的 init方法有一个flag的parse，会去解析参数，但参数不能重命名。如果依赖管理做的不好，出现多个glog目录，这些包在不同路径所以会走init方法，会发现flag重复了，影响使用。
> k8s使用自己的klog
## go程序性能分析 performance profiling
- CPU PROFILING： 在代码中添加 pprof.StartCPUProfile(f) 在defer中close掉。 然后找到文件所在地，这是一个二进制文件，需要用go tool pprof来查看这个file中的信息。执行之后，使用linux命令，如top就可以看到开销在哪里(哪个包哪个函数）了。
- memory profiling
- block profiling：goroutine细节，有无死锁
- goroutine profiling： 有哪些g，调用关系如何
- 针对http服务的pprof：net/http/pprof包，需要启动httpserver时把pprof handler注册到对应路径上。例如：
  
```go
mux := http.NewServeMux()
mux.HandleFunc("/debug/pprof/",pprof.Index)
err := http.ListenAndServe(":80", mux)
if err != nil{
  log.Fatal(err)
}
```

然后访问/debug/pprof/就能到pprof的控制台，可以执行看allocs block等信息。 如 `/debug/pprof/goroutine?debug=2` debug=2就是按照文本形式输出。
例如
- 里面的IO wait表示等待客户端请求  
- allocs里的[:]前面表示对象数，后面是大小。如果发生内存泄漏，可以从这里分析调用链。

> k8s大部分核心组件都带了pprof handler，可以通过url获取profile结果。

## k8s中的控制器模式
监听对象变化，做配置管理。在处理一件事情失败时，需要把对象重新放进队列里等待重试。放到队列里重试的等待时间指数型增长，避免频繁重试失败的任务。这是一种熔断机制。
## 排查方案
案例1:  
`perf top -p <pid>` 查找线程栈里的哪个调用占用了最多的cpu。例如 是gcDrain占了大头，那么考虑有较多对象需要回收。  
通过pprof分析内存占用情况：  
`curl xxx:xx/debug/pprof/heap?debug=2`  
查到syncProxyRules位于调用链中，随后调用getlocal address创建了大量对象。该函数是为了获取节点本机ip地址，获取方法时ip route命令获得所有local路由信息并转换（反序列化）成go struct并过滤掉ipvs0网口上的信息:  
`ip route show table local type local proto kernel`  
集群规模过大时，该命令返回5000条记录，每次函数调用都有大量对象生成。  
kube-proxy在处理每一个服务的时候都会调用该方法，每个服务都有虚ip，都绑在ipvs0的dev上，无论是ip address还是ip route都会返回很多信息。而这个调用又在for循环里，导致大量调用创建大量对象并让gc回收。gc跟不上对象产生的速度。解决方案是将调用提到for外面。
> 见pr #79444

案例2:  
k8s中有一个控制器：endpoint controller，是生产者消费者模式，消费者处理请求时可能http方式调用一个LBaas（openstack的接口）的 API 更新负载均衡配置。该控制器可能时不时不工作，没有变更配置，也没有打印相关日志。  
- 判断g被hang住。
- http client没有设置客户端超时时间，tcp需要双向协商，server出问题或网络出问题，resp回不来，client就会一直hang住。
- 解决方案：加入客户端超时控制